---
status: to-do
priority: normal
scheduled: 2026-01-22
dateCreated: 2026-01-22T09:17:25.358+08:00
dateModified: 2026-01-22T18:15:56.428+08:00
tags:
  - task
due: 2026-01-22
timeEntries:
  - startTime: 2026-01-22T18:15:56.428+08:00
    description: Work session
---
## 多源数据整合和动态透视分析
#  数据层
*负责连接数据源提取原始数据*
#### 1. 数据库连接 (db_utils.py)

- **实现方式**：使用 Streamlit 原生的 

    ```
    st.connection
    ```
    
     接口。
- **逻辑**：它读取 .streamlit/secrets.toml 中的配置（Host, User, Pass），建立一个持久化的 MySQL 连接池。
- **关键点**：使用了 
    
    ```
    ttl=0
    ```
    
     参数，确保每次查询都不使用过期的连接缓存，保证数据实时性。

#### 2. 核心查询逻辑 (

sql_queries.py)

- **实现方式**：编写复杂的 SQL 
    
    ```
    JOIN
    ```
    
     语句。
- **逻辑**：这是整个应用的地基。它通过 
    
    ```
    JOIN
    ```
    
     操作将散落在不同表中的数据串联起来：
    - ```
        t_order
        ```
        
         (订单主表) -> **基准**（谁买的？去哪？）。
    - ```
        t_order_sub
        ```
        
         (子订单表) -> **细分**（买了什么 SKU？）。
    - ```
        t_dispatcher_sub
        ```
        
         (配货表) -> **成本**（采购成本多少？）。
    - ```
        t_shipping
        ```
        
         (物流表) -> **时效**（什么时候发货？妥投没？）。
- **思考**：为什么是一条大 SQL？因为数据库层面的 
    
    ```
    JOIN
    ```
    
     效率远高于把数据拉到 Python 里再合并。

#### 3. 补录数据加载 (

load_google_data in 

app.py)

- **实现方式**：利用 Pandas 读取 Google Sheets 发布的 CSV 链接。
- **逻辑**：读取云端表格，用于修正或补充数据库中缺失的“物流实付成本”和“特殊商品成本”。